---
content_type: page
title: Readings
uid: 6897a3ff-dccb-36d9-9430-7f489f653321
---

Bertsekas = Bertsekas, Dimitri P. _Dynamic Programming and Optimal Control_. 2 vols. Belmont, MA: Athena Scientific, 2007. ISBN: 9781886529083.

Bertsekas and Tsitsiklis = Bertsekas, Dimitri P., and John N. Tsitsiklis. _Neuro-Dynamic Programming_. Belmont, MA: Athena Scientific, 1996. ISBN: 9781886529106.

| LECÂ # | TOPICS | READINGS |
| --- | --- | --- |
| 1 | Markov Decision Processes  {{< br >}}  {{< br >}}Finite-Horizon Problems: Backwards Induction  {{< br >}}  {{< br >}}Discounted-Cost Problems: Cost-to-Go Function, Bellman's Equation | Bertsekas Vol. 1, Chapter 1. |
| 2 | Value Iteration  {{< br >}}  {{< br >}}Existence and Uniqueness of Bellman's Equation Solution  {{< br >}}  {{< br >}}Gauss-Seidel Value Iteration | Bertsekas Vol. 2, Chapter 1.  {{< br >}}  {{< br >}}Bertsekas and Tsitsiklis, Chapter 2. |
| 3 | Optimality of Policies derived from the Cost-to-Go Function  {{< br >}}  {{< br >}}Policy Iteration  {{< br >}}  {{< br >}}Asynchronous Policy Iteration | Bertsekas Vol. 2, Chapter 1.  {{< br >}}  {{< br >}}Bertsekas and Tsitsiklis, Chapter 2. |
| 4 | Average-Cost Problems  {{< br >}}  {{< br >}}Relationship with Discounted-Cost Problems  {{< br >}}  {{< br >}}Bellman's Equation  {{< br >}}  {{< br >}}Blackwell Optimality | Bertsekas Vol. 2, Chapter 4. |
| 5 | Average-Cost Problems  {{< br >}}  {{< br >}}Computational Methods | Bertsekas Vol. 2, Chapter 4. |
| 6 | Application of Value Iteration to Optimization of Multiclass Queueing Networks  {{< br >}}  {{< br >}}Introduction to Simulation-based Methods Real-Time Value Iteration | Chen, R. R., and S. P. Meyn. "[Value Iteration and Optimization of Multiclass Queueing Networks](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.8423)."_Queueing Systems_ 32 (1999): 65-97.  {{< br >}}  {{< br >}}Bertsekas and Tsitsiklis, Chapter 5. |
| 7 | Q-Learning  {{< br >}}  {{< br >}}Stochastic Approximations | Bertsekas and Tsitsiklis, Chapters 4 and 5. |
| 8 | Stochastic Approximations: Lyapunov Function Analysis  {{< br >}}  {{< br >}}The ODE Method  {{< br >}}  {{< br >}}Convergence of Q-Learning | Bertsekas and Tsitsiklis, Chapters 4 and 5. |
| 9 | Exploration versus Exploitation: The Complexity of Reinforcement Learning | Kearns, M. , and S. Singh. "[Near-Optional Reinforcement Learning in Polynomial Time](http://www.cis.upenn.edu/~mkearns/papers/reinforcement.pdf)." _Machine Learning_ 49, no. 2 (Nov 2002): 209-232. |
| 10 | Introduction to Value Function Approximation  {{< br >}}  {{< br >}}Curse of Dimensionality  {{< br >}}  {{< br >}}Approximation Architectures | Bertsekas and Tsitsiklis, Chapter 6. |
| 11 | Model Selection and Complexity | Hastie, Tibshirani, and Friedmann. Chapter 7 in _The Elements of Statistical Learning_. New York: Springer, 2003. ISBN: 9780387952840. |
| 12 | Introduction to Value Function Approximation Algorithms  {{< br >}}  {{< br >}}Performance Bounds | Bertsekas and Tsitsiklis, Chapter 6. |
| 13 | Temporal-Difference Learning with Value Function Approximation | Bertsekas and Tsitsiklis, Chapter 6. |
| 14 | Temporal-Difference Learning with Value Function Approximation (cont.) | Bertsekas and Tsitsiklis, Chapter 6.  {{< br >}}  {{< br >}}de Farias, D. P., and B. Van Roy. "[On the Existence of Fixed Points for Approximate Value Iteration and Temporal-Difference Learning](http://dx.doi.org/10.1023/A:1004641123405)." |
| 15 | Temporal-Difference Learning with Value Function Approximation (cont.)  {{< br >}}  {{< br >}}Optimal Stopping Problems  {{< br >}}  {{< br >}}General Control Problems | Bertsekas and Tsitsiklis, Chapter 6.  {{< br >}}  {{< br >}}de Farias, D. P., and B. Van Roy. "[On the Existence of Fixed Points for Approximate Value Iteration and Temporal-Difference Learning](http://dx.doi.org/10.1023/A:1004641123405)."  {{< br >}}  {{< br >}}Bertsekas, Borkar, and Nedic. "[Improved temporal Difference Methods with Linear Function Approximation](http://onlinelibrary.wiley.com/doi/10.1002/9780470544785.ch9/summary)." |
| 16 | Approximate Linear Programming | de Farias, D. P., and B. Van Roy. "[The Linear Programming Approach to Approximate Dynamic Programming](http://www.mit.edu/~pucci/discountedLP.pdf)." |
| 17 | Approximate Linear Programming (cont.) | de Farias, D. P., and B. Van Roy. "[The Linear Programming Approach to Approximate Dynamic Programming](http://www.mit.edu/~pucci/discountedLP.pdf)." |
| 18 | Efficient Solutions for Approximate Linear Programming | de Farias D. P., and B. Van Roy. "[On Constraint Sampling in the Linear Programming Approach to Approximate Dynamic Programming](http://www.mit.edu/~pucci/sampling.pdf)."  {{< br >}}  {{< br >}}Calafiori, and Campi. "[Uncertain Convex Programs: Randomized Solutions and Confidence Levels](http://academic.research.microsoft.com/Publication/1744417/uncertain-convex-programs-randomized-solutions-and-confidence-levels)." |
| 19 | Efficient Solutions for Approximate Linear Programming: Factored MDPs | Guestrin, et al. "[Efficient Solution Algorithms for Factored MDPs](http://www-2.cs.cmu.edu/afs/cs/project/jair/pub/volume19/guestrin03a.pdf)."  {{< br >}}  {{< br >}}Schuurmans, and Patrascu. "[Direct Value Approximation for Factored MDPs](http://citeseer.ist.psu.edu/schuurmans01direct.html)." |
| 20 | Policy Search Methods | Marbach, and Tsitsiklis. "Simulation-Based Optimization of Markov Reward Processes." ([PDF](http://www.mit.edu/~jnt/Papers/J083-01-mar-MDP.pdf)) |
| 21 | Policy Search Methods (cont.) | Baxter, and Bartlett. "[Infinite-Horizon Policy-Gradient Estimation](http://www-2.cs.cmu.edu/afs/cs/project/jair/pub/volume15/baxter01a.pdf)." |
| 22 | Policy Search Methods for POMDPs  {{< br >}}  {{< br >}}Application: Call Admission Control  {{< br >}}  {{< br >}}Actor-Critic Methods | Baxter, and Bartlett. "[Infinite-Horizon Policy-Gradient Estimation](http://www-2.cs.cmu.edu/afs/cs/project/jair/pub/volume15/baxter01a.pdf)."  {{< br >}}  {{< br >}}Baxter, and Bartlett. "[Experiments with Infinite-Horizon Policy-Gradient Estimation](http://www-2.cs.cmu.edu/afs/cs/project/jair/pub/volume15/baxter01b.pdf)."  {{< br >}}  {{< br >}}Konda, and Tsitsiklis. "Actor-Critic Algorithms." ([PDF](http://www.mit.edu/~jnt/Papers/J094-03-kon-actors.pdf)) |
| 23 | Guest Lecture: Prof. Nick Roy  {{< br >}}  {{< br >}}Approximate POMDP Compression | Roy, and Gordon. "[Exponential Family PCA for Belief Compression in POMDPs](http://web.mit.edu/nickroy/www/papers/nips02.pdf)." |
| 24 | Policy Search Methods: PEGASUS  {{< br >}}  {{< br >}}Application: Helicopter Control | Ng, and Jordan. "[PEGASUS: A policy search method for large MDPs and POMDPs](http://www.robotics.stanford.edu/~ang/papers/uai00-pegasus.pdf)."  {{< br >}}  {{< br >}}Ng, et al. "[Autonomous Helicopter Flight via Reinforcement Learning](http://books.nips.cc/papers/files/nips16/NIPS2003_CN07.pdf)." 

Complementary Reading
---------------------

Even-Dar, and Mansour. "[Learning Rates for Q-Learning](http://dl.acm.org/citation.cfm?id=1005333).'' _Journal of Machine Learning Research_ 5 (2003): 1-25.

Barron. "Universal approximation bounds for superpositions of a sigmoidal function." _IEEE Transactions on Information Theory_ 39 (1993): 930-944.

Tesauro. "[Temporal-Difference Learning and TD-Gammon](https://dx.doi.org/10.1145/203330.203343)'' _Communications of the ACM_ 38, no. 3 (1995).