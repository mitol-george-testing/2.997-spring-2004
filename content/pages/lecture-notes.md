---
content_type: page
title: Lecture Notes
uid: 621c1e57-c07f-f26c-a822-25ef44476992
---

| LEC # | TOPICS | LECTURE NOTES |
| --- | --- | --- |
| 1 | Markov Decision Processes  {{< br >}}  {{< br >}}Finite-Horizon Problems: Backwards Induction  {{< br >}}  {{< br >}}Discounted-Cost Problems: Cost-to-Go Function, Bellman's Equation | ([PDF]({{< baseurl >}}/resources/lec_1_v2)) |
| 2 | Value Iteration  {{< br >}}  {{< br >}}Existence and Uniqueness of Bellman's Equation Solution  {{< br >}}  {{< br >}}Gauss-Seidel Value Iteration | ([PDF]({{< baseurl >}}/resources/lec_2_v1)) |
| 3 | Optimality of Policies derived from the Cost-to-Go Function  {{< br >}}  {{< br >}}Policy Iteration  {{< br >}}  {{< br >}}Asynchronous Policy Iteration | ([PDF]({{< baseurl >}}/resources/lec_3_v3)) |
| 4 | Average-Cost Problems  {{< br >}}  {{< br >}}Relationship with Discounted-Cost Problems  {{< br >}}  {{< br >}}Bellman's Equation  {{< br >}}  {{< br >}}Blackwell Optimality | ([PDF]({{< baseurl >}}/resources/lec_4_v1)) |
| 5 | Average-Cost Problems  {{< br >}}  {{< br >}}Computational Methods | ([PDF]({{< baseurl >}}/resources/lec_5_v1)) |
| 6 | Application of Value Iteration to Optimization of Multiclass Queueing Networks  {{< br >}}  {{< br >}}Introduction to Simulation-based Methods Real-Time Value Iteration | ([PDF]({{< baseurl >}}/resources/lec_6_v1)) |
| 7 | Q-Learning  {{< br >}}  {{< br >}}Stochastic Approximations | ([PDF]({{< baseurl >}}/resources/lec_7_v1)) |
| 8 | Stochastic Approximations: Lyapunov Function Analysis  {{< br >}}  {{< br >}}The ODE Method  {{< br >}}  {{< br >}}Convergence of Q-Learning | ([PDF]({{< baseurl >}}/resources/lec_8_v1)) |
| 9 | Exploration versus Exploitation: The Complexity of Reinforcement Learning | ([PDF]({{< baseurl >}}/resources/lec_9_v1)) |
| 10 | Introduction to Value Function Approximation  {{< br >}}  {{< br >}}Curse of Dimensionality  {{< br >}}  {{< br >}}Approximation Architectures | ([PDF]({{< baseurl >}}/resources/lec_10_v1)) |
| 11 | Model Selection and Complexity | ([PDF]({{< baseurl >}}/resources/lec_11_v1)) |
| 12 | Introduction to Value Function Approximation Algorithms  {{< br >}}  {{< br >}}Performance Bounds | ([PDF]({{< baseurl >}}/resources/lec_12_v11)) |
| 13 | Temporal-Difference Learning with Value Function Approximation | ([PDF]({{< baseurl >}}/resources/lec_13_v1)) |
| 14 | Temporal-Difference Learning with Value Function Approximation (cont.) | ([PDF]({{< baseurl >}}/resources/lec_14_v1)) |
| 15 | Temporal-Difference Learning with Value Function Approximation (cont.)  {{< br >}}  {{< br >}}Optimal Stopping Problems  {{< br >}}  {{< br >}}General Control Problems | ([PDF]({{< baseurl >}}/resources/lec_15_v1)) |
| 16 | Approximate Linear Programming | ([PDF]({{< baseurl >}}/resources/lec_16_v1)) |
| 17 | Approximate Linear Programming (cont.) | ([PDF]({{< baseurl >}}/resources/lec_17_v1)) |
| 18 | Efficient Solutions for Approximate Linear Programming | ([PDF]({{< baseurl >}}/resources/lec_18_v1)) |
| 19 | Efficient Solutions for Approximate Linear Programming: Factored MDPs | ([PDF]({{< baseurl >}}/resources/lec_19_v1)) |
| 20 | Policy Search Methods | ([PDF]({{< baseurl >}}/resources/lec_20_v1)) |
| 21 | Policy Search Methods (cont.) | ([PDF]({{< baseurl >}}/resources/lec_21_v1)) |
| 22 | Policy Search Methods for POMDPs  {{< br >}}  {{< br >}}Application: Call Admission Control  {{< br >}}  {{< br >}}Actor-Critic Methods | &nbsp; |
| 23 | Approximate POMDP Compression | &nbsp; |
| 24 | Policy Search Methods: PEGASUS  {{< br >}}  {{< br >}}Application: Helicopter Control |