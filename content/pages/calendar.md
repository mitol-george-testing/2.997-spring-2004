---
content_type: page
title: Calendar
uid: 4d2b58df-f19e-28d4-e2f1-ef7d996632ea
---

| LECÂ # | TOPICS | KEY DATES |
| --- | --- | --- |
| 1 | Markov Decision Processes  {{< br >}}  {{< br >}}Finite-Horizon Problems: Backwards Induction  {{< br >}}  {{< br >}}Discounted-Cost Problems: Cost-to-Go Function, Bellman's Equation |  |
| 2 | Value Iteration  {{< br >}}  {{< br >}}Existence and Uniqueness of Bellman's Equation Solution  {{< br >}}  {{< br >}}Gauss-Seidel Value Iteration |  |
| 3 | Optimality of Policies derived from the Cost-to-go Function  {{< br >}}  {{< br >}}Policy Iteration  {{< br >}}  {{< br >}}Asynchronous Policy Iteration | Problem set 1 out |
| 4 | Average-Cost Problems  {{< br >}}  {{< br >}}Relationship with Discounted-Cost Problems  {{< br >}}  {{< br >}}Bellman's Equation  {{< br >}}  {{< br >}}Blackwell Optimality | Problem set 1 due |
| 5 | Average-Cost Problems  {{< br >}}  {{< br >}}Computational Methods |  |
| 6 | Application of Value Iteration to Optimization of Multiclass Queueing Networks  {{< br >}}  {{< br >}}Introduction to Simulation-based Methods Real-Time Value Iteration | Problem set 2 out |
| 7 | Q-Learning  {{< br >}}  {{< br >}}Stochastic Approximations |  |
| 8 | Stochastic Approximations: Lyapunov Function Analysis  {{< br >}}  {{< br >}}The ODE Method  {{< br >}}  {{< br >}}Convergence of Q-Learning |  |
| 9 | Exploration versus Exploitation: The Complexity of Reinforcement Learning |  |
| 10 | Introduction to Value Function Approximation  {{< br >}}  {{< br >}}Curse of Dimensionality  {{< br >}}  {{< br >}}Approximation Architectures |  |
| 11 | Model Selection and Complexity | Problem set 3 out |
| 12 | Introduction to Value Function Approximation Algorithms  {{< br >}}  {{< br >}}Performance Bounds |  |
| 13 | Temporal-Difference Learning with Value Function Approximation |  |
| 14 | Temporal-Difference Learning with Value Function Approximation (cont.) |  |
| 15 | Temporal-Difference Learning with Value Function Approximation (cont.)  {{< br >}}  {{< br >}}Optimal Stopping Problems  {{< br >}}  {{< br >}}General Control Problems |  |
| 16 | Approximate Linear Programming | Problem set 4 out |
| 17 | Approximate Linear Programming (cont.) |  |
| 18 | Efficient Solutions for Approximate Linear Programming |  |
| 19 | Efficient Solutions for Approximate Linear Programming: Factored MDPs |  |
| 20 | Policy Search Methods | Problem set 5 out |
| 21 | Policy Search Methods (cont.) |  |
| 22 | Policy Search Methods for POMDPs  {{< br >}}  {{< br >}}Application: Call Admission Control  {{< br >}}  {{< br >}}Actor-Critic Methods |  |
| 23 | Guest Lecture: Prof. Nick Roy  {{< br >}}  {{< br >}}Approximate POMDP Compression |  |
| 24 | Policy Search Methods: PEGASUS  {{< br >}}  {{< br >}}Application: Helicopter Control |